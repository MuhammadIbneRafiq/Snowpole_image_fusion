% ==========================================================
% Paper draft (LaTeX) for an Information Fusion conference submission
% Topic: Effect of Image Fusion on Snow-Pole Detection
% NOTE: This is a complete, editable draft with placeholders for your final numbers.
% ==========================================================

\documentclass[10pt,conference]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{xcolor}

\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\title{On the Effect of Image Fusion for Robust Snow-Pole Detection Under Nordic Winter Conditions}

\author{
\IEEEauthorblockN{Durga Prasad Bavirisetti\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}University of G\"avle, Sweden \\
durga.prasad.bavirisetti@hig.se}
}


\begin{document}
\maketitle

\begin{abstract}

Reliable detection of snow poles is essential for infrastructure-assisted perception and localization in Nordic winter conditions, where visibility is frequently degraded by snow cover, low illumination, and complex background clutter. While single-modality sensing can be effective in nominal scenarios, its robustness often deteriorates under adverse winter conditions. This paper presents a \emph{preliminary study} on the effect of multi-modal image fusion for snow-pole detection, focusing on baseline and classical fusion strategies. Using three co-registered 16-bit image modalities (\emph{signal}, \emph{reflect}, and \emph{near-infrared}) derived from a winter driving dataset, we evaluate a set of representative fusion methods, including pixel-level fusion (average, max/min, weighted averaging), feature-space fusion (PCA), multi-scale fusion (Laplacian pyramid), and an adaptive edge-preserving filtering-based approach. The fused representations are assessed for downstream detection using a YOLOv11-based detector under controlled experimental settings. The results indicate that image fusion can improve detection robustness in challenging scenes; however, the gains are highly dependent on the chosen fusion operator and the underlying modality statistics. This study provides early insights into the benefits and limitations of baseline fusion techniques and establishes a foundation for future, more comprehensive investigations using state-of-the-art and deep learningâ€“based fusion approaches.
\end{abstract}



\begin{IEEEkeywords}
Image fusion, multi-modal sensing, infrastructure perception, snow-pole detection, Nordic winter, LiDAR imaging, near-infrared, robustness.
\end{IEEEkeywords}

% ----------------------------------------------------------
\section{Introduction}
Snow poles are stable roadside landmarks widely deployed across Nordic regions to indicate road boundaries under heavy snow. For autonomous driving and advanced driver assistance, robust detection of snow poles offers a machine-perceivable cue when lane markings are obscured and GNSS is degraded. However, detection under winter conditions is challenging due to:
(i) reduced contrast and illumination variability,
(ii) snow-covered scenes with weak texture,
(iii) cluttered backgrounds (trees, posts, fences),
and (iv) sensor- and modality-specific artifacts.

Multi-modal sensing can mitigate some of these challenges by providing complementary information. In particular, co-registered modalities such as \emph{signal}, \emph{reflect}, and \emph{near-IR} often capture different radiometric responses of the same scene. A natural question is whether \emph{image fusion} can improve snow-pole detection by consolidating complementary cues and suppressing modality-specific noise.

This paper studies the \emph{effect of image fusion on snow-pole detection}, focusing on practical, reproducible fusion operators that can be integrated into perception pipelines. We address the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} Which fusion techniques improve snow-pole detection accuracy under winter conditions?
    \item \textbf{RQ2:} How does fusion influence detection robustness across different scene types (open roads, forested, urban)?
    \item \textbf{RQ3:} What are the failure modes of fusion for highly saturated or low-dynamic-range winter imagery?
\end{itemize}

\textbf{Contributions.} (1) A controlled benchmark of multiple fusion methods for co-registered 16-bit modalities used for snow-pole detection. (2) A systematic evaluation protocol linking fusion outputs to downstream detector performance. (3) An empirical analysis and actionable guidelines on selecting fusion methods for winter infrastructure detection.

% ----------------------------------------------------------
\section{Related Work}
\subsection{Image Fusion}
Image fusion can be performed at the pixel, feature, or decision level. Classical pixel-level fusion includes averaging and max-selection, while multi-scale fusion (e.g., Laplacian pyramids) fuses details across spatial frequencies. Statistical fusion such as PCA maps stacked modalities into principal components, often emphasizing variance-dominant cues. Adaptive filtering-based fusion uses local activity measures to weight modalities, frequently improving edge preservation.

\subsection{Winter Perception and Roadside Landmark Detection}
Winter perception literature highlights challenges due to snow occlusion and low contrast. Roadside landmarks (poles, posts, signs) can serve as robust cues. However, the interaction between fusion and detector learning is non-trivial: fusion may improve signal-to-noise in some regions but also distort textures or dynamic range, affecting detector generalization.

% ----------------------------------------------------------
\section{Problem Formulation}
Let $\{I_k\}_{k=1}^{K}$ denote $K$ co-registered input images of the same scene captured by different modalities. In this work, $K=3$ with:
\[
I_1 = I^{(\text{signal})},\quad
I_2 = I^{(\text{reflect})},\quad
I_3 = I^{(\text{nearIR})},
\]
stored as 16-bit images (mode \texttt{I;16}) of size $H\times W$.

An image fusion operator $\mathcal{F}(\cdot)$ produces a fused image:
\[
I^{(f)} = \mathcal{F}(I_1, I_2, I_3).
\]
We evaluate the effect of $\mathcal{F}$ on downstream snow-pole detection by training and testing the same detector architecture using fused images as input and comparing to single-modality baselines.

% ----------------------------------------------------------
\section{Fusion Methods}
We consider the following fusion methods. All operations are performed in 16-bit space to preserve dynamic range; any 8-bit rendering is used only for visualization.

\subsection{Pixel-level Baselines}
\textbf{Average fusion:}
\begin{equation}
I^{(f)}(x) = \frac{1}{K}\sum_{k=1}^{K} I_k(x).
\end{equation}

\textbf{Max fusion:}
\begin{equation}
I^{(f)}(x) = \max_{k} I_k(x).
\end{equation}

\textbf{Min fusion:}
\begin{equation}
I^{(f)}(x) = \min_{k} I_k(x).
\end{equation}

\textbf{Weighted averaging:}
\begin{equation}
I^{(f)}(x) = \sum_{k=1}^{K} \alpha_k I_k(x), \quad \sum_k \alpha_k = 1.
\end{equation}
We use $\alpha = [0.33,\,0.34,\,0.33]$ as a near-uniform baseline.

\subsection{PCA Fusion}
We stack modalities into a vector per pixel and project onto the dominant principal component:
\begin{equation}
\mathbf{i}(x) = [I_1(x), I_2(x), I_3(x)]^\top,\quad
I^{(f)}(x) = \mathbf{v}_1^\top(\mathbf{i}(x)-\boldsymbol{\mu}),
\end{equation}
where $\mathbf{v}_1$ is the first eigenvector of the covariance of modality vectors and $\boldsymbol{\mu}$ is the mean vector. The resulting component is linearly scaled back to 16-bit for storage.

\subsection{Multi-Scale Laplacian Pyramid Fusion}
We build Laplacian pyramids for each modality. For detail layers, we select coefficients with the largest absolute magnitude (activity selection), and average the top residual layer:
\begin{align}
L_k^\ell &= G_k^\ell - \uparrow(G_k^{\ell+1}), \\
L_f^\ell(x) &= L_{k^\ast}^\ell(x), \quad
k^\ast = \arg\max_k |L_k^\ell(x)|,\\
G_f^{L}(x) &= \frac{1}{K}\sum_k G_k^{L}(x),
\end{align}
followed by pyramid reconstruction.

\subsection{Adaptive Filtering-Based Fusion (MATLAB-style)}
Inspired by adaptive weighting via local activity measures, we define, for two modalities $I_a$ and $I_b$:
\begin{align}
B_a &= \text{BoxFilter}(I_a), \quad D_a = I_a - B_a,\\
S_a &= (B_a - \text{Median}(I_a))^2,
\end{align}
and similarly for $I_b$. The weights are:
\begin{equation}
w_a = \frac{S_a}{S_a + S_b + \epsilon}, \quad
w_b = \frac{S_b}{S_a + S_b + \epsilon},
\end{equation}
and the fused output:
\begin{equation}
I_f = w_a D_a + w_b D_b + \frac{1}{2}(B_a + B_b).
\end{equation}
To fuse three modalities, we apply the operator sequentially:
\[
I_{ab} = \mathcal{F}_{\text{adapt}}(I_a, I_b),\quad
I_f = \mathcal{F}_{\text{adapt}}(I_{ab}, I_c).
\]
We use $N=3$ median window size and $M=35$ box filter size as in the reference implementation.

% ----------------------------------------------------------
% \section{Experimental Setup}
% \subsection{Dataset}
% We use a winter driving dataset with co-registered 16-bit modalities (signal, reflect, near-IR) and bounding-box annotations for snow poles. The dataset contains diverse Nordic scenes including open roads, forested sections, and urban areas. \todo{Add dataset name, size, train/val/test split, and annotation format.}

% \subsection{Preprocessing}
% All modalities are assumed co-registered. Fusion is performed in 16-bit intensity space. For detector input, we consider:
% \begin{itemize}
%     \item \textbf{Single-modality baseline:} train/test on each of $\{I_1, I_2, I_3\}$.
%     \item \textbf{Fused modality:} train/test on $I^{(f)}$ for each fusion method.
% \end{itemize}
% To isolate the effect of fusion, we keep the detector architecture and training schedule fixed across experiments.

% \subsection{Detector and Training Protocol}
% We employ a single-stage detector (\todo{e.g., YOLOv5/YOLOv8/YOLOv11}) with identical hyperparameters for all runs. We report detection performance using:
% \begin{itemize}
%     \item Precision, Recall, F1-score
%     \item mAP@0.5 and mAP@[0.5:0.95]
% \end{itemize}
% We also analyze robustness by reporting results per scene category when available.

% \subsection{Implementation Details}
% Fusion and visualization are implemented in Python. All fused images are saved as 16-bit PNG/TIFF to preserve dynamic range. Visualization is done without forcing display scaling (no fixed \texttt{vmin}/\texttt{vmax}) to avoid misleading comparisons. \todo{Add runtime, hardware, and software versions.}

% % ----------------------------------------------------------
% \section{Results}
% \subsection{Quantitative Detection Performance}
% Table~\ref{tab:main-results} compares detection metrics across single-modality baselines and fusion methods.

% \begin{table}[t]
% \centering
% \caption{Snow-pole detection results on the test set. Best per column in bold.}
% \label{tab:main-results}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Input} & \textbf{Prec.} & \textbf{Rec.} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95}\\
% \midrule
% Signal (I;16)            & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Reflect (I;16)           & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Near-IR (I;16)           & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% \midrule
% Avg fusion               & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Max fusion               & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Min fusion               & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Weighted avg             & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% PCA fusion               & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Laplacian pyramid        & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% Adaptive (2-stage)       & \todo{ } & \todo{ } & \todo{ } & \todo{ } \\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Qualitative Analysis}
% Fig.~\ref{fig:qual} illustrates representative fused outputs and corresponding detection results. We observe that:
% (i) max fusion often enhances high-intensity structures but can amplify saturation,
% (ii) Laplacian fusion preserves edges while limiting over-smoothing,
% (iii) PCA may over-emphasize a dominant modality and wash out complementary cues,
% (iv) adaptive fusion can improve local contrast around pole-like structures but may introduce halo artifacts if modality noise is high.

% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{figures/fusion_rows_multiple_techniques_plus_matlab.png}
% \caption{Row-wise visualization of source modalities and fused outputs (16-bit).}
% \label{fig:qual}
% \end{figure}

% \subsection{Scene-Wise Robustness}
% We further group test images into scene categories (\todo{forest/open/urban}) and evaluate per-group performance to identify conditions where fusion is most beneficial. \todo{Add table/plot and discussion.}
\section{Experimental Setup}

\subsection{Dataset}
The experiments are conducted on a winter driving dataset captured under Nordic conditions, containing co-registered 16-bit image modalities (\emph{signal}, \emph{reflect}, and \emph{near-infrared}). The dataset comprises a total of \textbf{1,954 manually annotated images}, each labeled with bounding boxes corresponding to snow poles.

The dataset is split following a standard \textbf{70/20/10} protocol:
\begin{itemize}
    \item \textbf{Training set:} 1,367 images
    \item \textbf{Validation set:} 390 images
    \item \textbf{Test set:} 197 images
\end{itemize}
This split is kept fixed across all experiments to ensure a fair comparison between different fusion strategies.

\subsection{Fusion Preprocessing}
All three modalities are assumed to be spatially aligned. Image fusion is performed directly in the native \textbf{16-bit intensity space} (mode \texttt{I;16}) to avoid loss of dynamic range. The fused outputs are stored as 16-bit images and used directly as input to the detector. Any 8-bit conversion is performed solely for visualization purposes.

The following fusion methods are evaluated:
\begin{itemize}
    \item Average fusion
    \item Max fusion
    \item Min fusion
    \item Weighted average fusion
    \item PCA-based fusion
    \item Laplacian pyramid (multi-scale) fusion
    \item Adaptive filtering-based fusion (MATLAB-style, two-stage)
\end{itemize}

\subsection{Detector and Training Protocol}
Snow-pole detection is performed using \textbf{YOLOv11}, a modern single-stage object detector optimized for efficiency and accuracy. To isolate the effect of image fusion, the same detector architecture and training configuration are used for all experiments.

All models are trained from scratch using identical hyperparameters, data augmentation strategies, and training schedules. The detector is trained on either:
\begin{enumerate}
    \item A single input modality (signal, reflect, or near-IR), or
    \item A fused image produced by one of the fusion methods.
\end{enumerate}

\subsection{Evaluation Metrics}
Detection performance is evaluated on the held-out test set using standard object detection metrics:
\begin{itemize}
    \item \textbf{Precision}
    \item \textbf{Recall}
    \item \textbf{mean Average Precision (mAP)}
\end{itemize}
These metrics quantify the impact of fusion on detection accuracy and robustness.

---

\section{Results}

\subsection{Quantitative Results}
Table~\ref{tab:detection-results} summarizes the snow-pole detection performance obtained using single-modality inputs and various fusion strategies.

\begin{table}[t]
\centering
\caption{Snow-pole detection performance using YOLOv12.}
\label{tab:detection-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Input} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} \\
\midrule
Signal (I; 16-bit)      & -- & -- & -- & -- \\
Reflect (I; 16-bit)     & -- & -- & -- & -- \\
Near-IR (I; 16-bit)     & -- & -- & -- & -- \\
Combination 4 (I; 16-bit) & -- & -- & -- & -- \\
\multicolumn{5}{l}{\quad (R = Near-IR, G = Signal, B = Reflectance)} \\
\midrule
Average fusion            & -- & -- & -- & -- \\
Max fusion                & -- & -- & -- & -- \\
Min fusion                & -- & -- & -- & -- \\
Weighted average fusion   & -- & -- & -- & -- \\
PCA fusion                & -- & -- & -- & -- \\
Laplacian pyramid fusion  & -- & -- & -- & -- \\
Adaptive fusion (two-stage) & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Qualitative Analysis}
Fig.~\ref{fig:fusion-qualitative} presents qualitative examples of the source modalities and the corresponding fused outputs generated using different fusion techniques. These examples are drawn directly from the experimental dataset and illustrate the visual characteristics introduced by each fusion strategy.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/Image00240_fusion_rows_multiple_techniques_plus_matlab.png}
\caption{Row-wise qualitative comparison of source modalities (signal, reflect, near-IR) and fused images generated using different fusion methods. All images are shown in native 16-bit format without forced intensity scaling.}
\label{fig:fusion-qualitative}
\end{figure}

Visual inspection reveals that simple fusion strategies such as averaging and max fusion often amplify saturation effects, whereas multi-scale fusion better preserves edge information associated with thin pole-like structures. The adaptive filtering-based fusion enhances local contrast around snow poles but may introduce smoothing artifacts in homogeneous regions.

---

\section{Discussion}
The experimental results indicate that image fusion can positively impact snow-pole detection performance; however, the extent of improvement depends strongly on the fusion strategy. While naive fusion methods provide limited gains, multi-scale and adaptive fusion approaches demonstrate improved robustness under challenging winter conditions.

Importantly, fusion does not universally improve detection. In scenes with highly saturated or low-dynamic-range inputs, fusion can suppress discriminative features or introduce artifacts that negatively affect detector learning. These observations highlight the need for careful selection and validation of fusion operators in winter perception pipelines.

---
% ----------------------------------------------------------
\section{Discussion}
\subsection{When Fusion Helps}
Fusion improves detection when modalities are complementary, e.g., one modality offers stable pole contrast while another suppresses background clutter. Multi-scale fusion is particularly effective when poles are thin structures whose detectability depends on edge strength at multiple spatial scales.

\subsection{When Fusion Saturates or Hurts}
In winter imagery with strong saturation or narrow dynamic range, naive averaging can reduce discriminative contrast. Max fusion can amplify noise and produce overly bright fused images that distort the detector's learned features. PCA may collapse to the variance-dominant modality and discard useful complementary cues.

\subsection{Practical Recommendations}
Based on empirical findings, we recommend:
\begin{itemize}
    \item Prefer multi-scale fusion (e.g., Laplacian pyramid) when poles are thin and edge cues matter.
    \item Validate fusion output statistics (histograms, saturation ratios) before detector training.
    \item Avoid fixed display scaling when reporting qualitative results; show consistent, documented visualization.
    \item Consider training-time augmentation to reduce sensitivity to fusion-induced contrast changes.
\end{itemize}

% ----------------------------------------------------------
\section{Conclusion}
This paper analyzed the effect of image fusion on snow-pole detection under Nordic winter conditions using three co-registered 16-bit modalities. Across multiple fusion operators, we found that the benefit of fusion depends on modality complementarity and the fusion operator's ability to preserve pole-relevant structures while suppressing noise. Multi-scale and adaptive fusion methods are promising for robustness, whereas naive fusion can saturate or smooth discriminative cues. Future work will explore learning-based fusion with detector-aware objectives and extend evaluation to joint detection-localization tasks in GNSS-limited environments.

% ----------------------------------------------------------
\section*{Acknowledgment}
\todo{Add funding and project acknowledgments.}

% ----------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
